{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxTMDJsRC+nf/jE0gHFmIY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakul/MongoDB-AI-Resources/blob/main/Langchain%2BMongoDB_Parent_document_retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Author: Prakul agarwal\n",
        "# Install prerequisites dependencies\n"
      ],
      "metadata": {
        "id": "3CaG54pnNhqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain pypdf pymongo openai python-dotenv tiktoken"
      ],
      "metadata": {
        "id": "HwQZnho4m5IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the environment"
      ],
      "metadata": {
        "id": "H0zbGavtNpMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pymongo import MongoClient\n",
        "\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Add an environment file to the notebook root directory called .env with MONGO_URI=\"xxx\" to load these envvars\n",
        "\n",
        "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
        "MONGO_URI = os.environ[\"MONGO_URI\"]\n",
        "DB_NAME = \"langchain-test-3\"\n",
        "COLLECTION_NAME = \"test\"\n",
        "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"default\"\n",
        "EMBEDDING_FIELD_NAME = \"embedding\"\n",
        "\n",
        "client = MongoClient(MONGO_URI)\n",
        "db = client[DB_NAME]\n",
        "MONGODB_COLLECTION = db[COLLECTION_NAME]"
      ],
      "metadata": {
        "id": "JWl8u9gdyBGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREPARE DATA"
      ],
      "metadata": {
        "id": "_NbBxI940_yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"https://arxiv.org/pdf/2303.08774.pdf\")\n",
        "#data = loader.load_and_split()\n",
        "data = loader.load()\n",
        "docs = loader.load_and_split()\n"
      ],
      "metadata": {
        "id": "bLosUpvDmDOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def produce_sentences(docs):\n",
        "    docs[1].page_content.split('.')\n",
        "    new_docs = []\n",
        "    for doc in docs:\n",
        "        for sentence in doc.page_content.split('.'):\n",
        "            if len(sentence) < 2:\n",
        "                continue\n",
        "            new_doc = copy.deepcopy(doc)\n",
        "            new_doc.page_content = sentence\n",
        "            new_doc.metadata['doc_level'] = 'sentence'\n",
        "            new_docs.append(new_doc)\n",
        "        doc.metadata['doc_level'] = 'page'\n",
        "\n",
        "    return new_docs"
      ],
      "metadata": {
        "id": "n8m2aQYxyhig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def parent_child_splitter(data):\n",
        "    parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
        "    # This text splitter is used to create the child documents\n",
        "    # It should create documents smaller than the parent\n",
        "    child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
        "    parent_docs = parent_splitter.split_documents(data)\n",
        "    child_docs = child_splitter.split_documents(data)\n",
        "    for parent_doc in parent_docs:\n",
        "        parent_doc.metadata[\"doc_level\"] = \"parent\"\n",
        "    for child_doc in child_docs:\n",
        "        child_doc.metadata[\"doc_level\"] = \"child\"\n",
        "    return parent_docs, child_docs\n",
        "\n"
      ],
      "metadata": {
        "id": "8Z6mtKcguihi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parent_docs, child_docs  = parent_child_splitter(data)"
      ],
      "metadata": {
        "id": "KtZ0lOH1wX42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(parent_docs[0])\n",
        "len(parent_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_lu8fUPyI4K",
        "outputId": "2b431725-bf73-4624-81ff-6ca16805678f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated\\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\\nbased model pre-trained to predict the next token in a document. The post-training\\nalignment process results in improved performance on measures of factuality and\\nadherence to desired behavior. A core component of this project was developing\\ninfrastructure and optimization methods that behave predictably across a wide\\nrange of scales. This allowed us to accurately predict some aspects of GPT-4’s\\nperformance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.\\nThis contrasts with GPT-3.5, which scores in the bottom 10%.' metadata={'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 0, 'doc_level': 'parent'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "207"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(child_docs[0])\n",
        "len(child_docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F05WZxq53XvX",
        "outputId": "bb7683b5-971d-40a0-faf3-7c99bd1c1564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated' metadata={'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 0, 'doc_level': 'child'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1393"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUrRHjYumSh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INSERT DATA"
      ],
      "metadata": {
        "id": "-cMi4-JR49-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
        "\n",
        "# insert the documents in MongoDB Atlas Vector Search\n",
        "x = MongoDBAtlasVectorSearch.from_documents(\n",
        "     documents=parent_docs+child_docs, embedding=OpenAIEmbeddings(disallowed_special=()), collection=MONGODB_COLLECTION, index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
        " )\n"
      ],
      "metadata": {
        "id": "jg8Q1pwY4_on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATE INDEX\n",
        "\n",
        " Create an Atlas search index with the  definition given below, using\n",
        "(option a) pymongo driver\n",
        "(option b) Atlas UI -> Search -> JSON Editor\n",
        " https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/\n"
      ],
      "metadata": {
        "id": "F7v5H-48MW18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "{\n",
        "  \"mappings\": {\n",
        "    \"dynamic\": true,\n",
        "    \"fields\": {\n",
        "      \"embedding\": {\n",
        "        \"dimensions\": 1536,\n",
        "        \"similarity\": \"cosine\",\n",
        "        \"type\": \"knnVector\"\n",
        "      },\n",
        "      \"doc_level\": [\n",
        "        {\n",
        "          \"type\": \"token\"\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "dCnjAd-_0oB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "MONGODB_COLLECTION.create_search_index(\n",
        "    {\"definition\":\n",
        "        {\"mappings\":\n",
        "         {\"dynamic\": True,\n",
        "          \"fields\": {\n",
        "            EMBEDDING_FIELD_NAME : {\n",
        "                \"dimensions\": 1536,\n",
        "                \"similarity\": \"cosine\",\n",
        "                \"type\": \"knnVector\"\n",
        "                },\n",
        "            \"doc_level\": {\n",
        "                \"type\": \"token\"\n",
        "            }\n",
        "            }}},\n",
        "     \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "eE8ciqaa7JLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA QUERY"
      ],
      "metadata": {
        "id": "I0yiKlx9MRXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
        "\n",
        "vector_search = MongoDBAtlasVectorSearch.from_connection_string(\n",
        "    MONGO_URI,\n",
        "    DB_NAME + \".\" + COLLECTION_NAME,\n",
        "    OpenAIEmbeddings(disallowed_special=()),\n",
        "    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME\n",
        ")\n"
      ],
      "metadata": {
        "id": "CRP1c2PTX_PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now call the vector search functionality on the smaller 'child' chunks\n",
        "sub_docs = vector_search.similarity_search(\"gpt-4 compute requirements\", k=5, pre_filter=\n",
        "            {\n",
        "        \"doc_level\": { \"$eq\": \"child\"}\n",
        "    }, post_filter_pipeline=[{\"$project\": {\"embedding\": 0}}]\n",
        "                                           )\n",
        "for doc in sub_docs:\n",
        "    print(doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlIcAUzK8JdV",
        "outputId": "bb6a250a-555f-416b-a414-8e5e7b716fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='gpt3.5Figure 4. GPT performance on academic and professional exams. In each case, we simulate the\\nconditions and scoring of the real exam. Exams are ordered from low to high based on GPT-3.5\\nperformance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\\nlower end of the range of percentiles, but this creates some artifacts on the AP exams which have very' metadata={'_id': ObjectId('65430b6f179f5b5bbf292c55'), 'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 5, 'doc_level': 'child'}\n",
            "page_content='GPT-4 model, but they did not have the ability to ﬁne-tune it. They also did not have access to the\\nﬁnal version of the model that we deployed. The ﬁnal version has capability improvements relevant\\nto some of the factors that limited the earlier models power-seeking abilities, such as longer context\\nlength, and improved problem-solving abilities as in some cases we /quotesingle.ts1ve observed.' metadata={'_id': ObjectId('65430b7c179f5b5bbf292f33'), 'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 54, 'doc_level': 'child'}\n",
            "page_content='to test GPT-4’s ability to aid in computer vulnerability discovery, assessment, and exploitation.\\nThey found that GPT-4 could explain some vulnerabilities if the source code was small enough\\nto ﬁt in the context window, just as the model can explain other source code. However, GPT-4\\nperformed poorly at building exploits for the vulnerabilities that were identiﬁed.' metadata={'_id': ObjectId('65430b7b179f5b5bbf292f1d'), 'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 53, 'doc_level': 'child'}\n",
            "page_content='performance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\\nlower end of the range of percentiles, but this creates some artifacts on the AP exams which have very\\nwide scoring bins. For example although GPT-4 attains the highest possible score on AP Biology (5/5),' metadata={'_id': ObjectId('65430b6f179f5b5bbf292c56'), 'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 5, 'doc_level': 'child'}\n",
            "page_content='2 GPT-4 Observed Safety Challenges\\nGPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\\ncoding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\\nalso present new safety challenges, which we highlight in this section.\\nWe conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations' metadata={'_id': ObjectId('65430b79179f5b5bbf292e64'), 'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 43, 'doc_level': 'child'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now call the vector search functionality on the larger 'parent' chunks\n",
        "sub_docs = vector_search.similarity_search(\"gpt-4 compute requirements\", k=5, pre_filter=\n",
        "            {\n",
        "        \"doc_level\": { \"$eq\": \"parent\"}\n",
        "    }, post_filter_pipeline=[{\"$project\": {\"embedding\": 0}}]\n",
        "                                           )\n",
        "for doc in sub_docs:\n",
        "    print(doc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8149CrPagwLt",
        "outputId": "14e969f4-cbc3-4abf-9709-4b478309f394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Observed\\nPrediction\\ngpt-4\\n100p 10n 1µ 100µ 0.01 1\\nCompute1.02.03.04.05.06.0Bits per wordOpenAI codebase next word predictionFigure 1. Performance of GPT-4 and smaller models. The metric is ﬁnal loss on a dataset derived\\nfrom our internal codebase. This is a convenient, large dataset of code tokens which is not contained in\\nthe training set. We chose to look at loss because it tends to be less noisy than other measures across\\ndifferent amounts of training compute. A power law ﬁt to the smaller models (excluding GPT-4) is\\nshown as the dotted line; this ﬁt accurately predicts GPT-4’s ﬁnal loss. The x-axis is training compute\\nnormalized so that GPT-4 is 1.\\nObserved\\nPrediction\\ngpt-4\\n1µ 10µ 100µ 0.001 0.01 0.1 1\\nCompute012345– Mean Log Pass RateCapability prediction on 23 coding problems\\nFigure 2. Performance of GPT-4 and smaller models. The metric is mean log pass rate on a subset of\\nthe HumanEval dataset. A power law ﬁt to the smaller models (excluding GPT-4) is shown as the dotted\\nline; this ﬁt accurately predicts GPT-4’s performance. The x-axis is training compute normalized so that\\nGPT-4 is 1.\\n3' metadata={'_id': ObjectId('65430b6a179f5b5bbf292b43'), 'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 2, 'doc_level': 'parent'}\n",
            "page_content='GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated\\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\\nbased model pre-trained to predict the next token in a document. The post-training\\nalignment process results in improved performance on measures of factuality and\\nadherence to desired behavior. A core component of this project was developing\\ninfrastructure and optimization methods that behave predictably across a wide\\nrange of scales. This allowed us to accurately predict some aspects of GPT-4’s\\nperformance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.\\nThis contrasts with GPT-3.5, which scores in the bottom 10%.' metadata={'_id': ObjectId('65430b6a179f5b5bbf292b3e'), 'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 0, 'doc_level': 'parent'}\n",
            "page_content='For example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.\\nThis contrasts with GPT-3.5, which scores in the bottom 10%.\\nOn a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models\\nand most state-of-the-art systems (which often have benchmark-speciﬁc training or hand-engineering).\\nOn the MMLU benchmark [ 35,36], an English-language suite of multiple-choice questions covering\\n57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but\\nalso demonstrates strong performance in other languages. On translated variants of MMLU, GPT-4\\nsurpasses the English-language state-of-the-art in 24 of 26 languages considered. We discuss these\\nmodel capability results, as well as model safety improvements and results, in more detail in later\\nsections.\\nThis report also discusses a key challenge of the project, developing deep learning infrastructure and\\noptimization methods that behave predictably across a wide range of scales. This allowed us to make\\npredictions about the expected performance of GPT-4 (based on small runs trained in similar ways)\\nthat were tested against the ﬁnal run to increase conﬁdence in our training.\\nDespite its capabilities, GPT-4 has similar limitations to earlier GPT models [ 1,37,38]: it is not fully\\nreliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn\\n∗Please cite this work as “OpenAI (2023)\". Full authorship contribution statements appear at the end of the\\ndocument. Correspondence regarding this technical report can be sent to gpt4-report@openai.comarXiv:2303.08774v3  [cs.CL]  27 Mar 2023' metadata={'_id': ObjectId('65430b6a179f5b5bbf292b3f'), 'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 0, 'doc_level': 'parent'}\n",
            "page_content='Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98' metadata={'_id': ObjectId('65430b6f179f5b5bbf292c09'), 'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 97, 'doc_level': 'parent'}\n",
            "page_content='5 Limitations\\nDespite its capabilities, GPT-4 has similar limitations as earlier GPT models. Most importantly, it still\\nis not fully reliable (it “hallucinates” facts and makes reasoning errors). Great care should be taken\\nwhen using language model outputs, particularly in high-stakes contexts, with the exact protocol\\n(such as human review, grounding with additional context, or avoiding high-stakes uses altogether)\\nmatching the needs of speciﬁc applications. See our System Card for details.\\nGPT-4 signiﬁcantly reduces hallucinations relative to previous GPT-3.5 models (which have them-\\nselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our\\nlatest GPT-3.5 on our internal, adversarially-designed factuality evaluations (Figure 6).\\nlearning technology writing history math science recommendation code business0%20%40%60%80%\\nCategoryAccuracy\\nInternal factual eval by category\\nchatgpt-v2\\nchatgpt-v3\\nchatgpt-v4\\ngpt-4\\nFigure 6. Performance of GPT-4 on nine internal adversarially-designed factuality evaluations. Accuracy\\nis shown on the y-axis, higher is better. An accuracy of 1.0 means the model’s answers are judged to\\nbe in agreement with human ideal responses for all questions in the eval. We compare GPT-4 to three\\nearlier versions of ChatGPT [ 64] based on GPT-3.5; GPT-4 improves on the latest GPT-3.5 model by 19\\npercentage points, with signiﬁcant gains across all topics.\\nGPT-4 makes progress on public benchmarks like TruthfulQA [ 66], which tests the model’s ability to\\nseparate fact from an adversarially-selected set of incorrect statements (Figure 7). These questions\\nare paired with factually incorrect answers that are statistically appealing. The GPT-4 base model is\\nonly slightly better at this task than GPT-3.5; however, after RLHF post-training we observe large\\nimprovements over GPT-3.5.9Table 4 shows both a correct and an incorrect answer. GPT-4 resists' metadata={'_id': ObjectId('65430b6a179f5b5bbf292b4e'), 'source': '/tmp/tmpfbhs4lhc/tmp.pdf', 'page': 9, 'doc_level': 'parent'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parent child retrieval\n",
        "\n",
        "Now we want to search on the smaller 'child' chunks but return the larger 'parent' chunks\n",
        "\n",
        "To perform this we will be performing an `left outer join` on our collection to find the parent chunk\n",
        "corresponding to a child chunk such that it matches the\n",
        "`page number`. This `left outer join` is performed via the [$lookup operator in MongoDB](https://www.mongodb.com/docs/manual/reference/operator/aggregation/lookup/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9oMSxyhF4sRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now we want to search on the smaller 'child' chunks but return the larger 'parent' chunks\n",
        "\"\"\"\n",
        "To perform this we will be performing an `left outer join` on our collection to find the parent chunk\n",
        "corresponding to a child chunk such that it matches the\n",
        "`page number`. This `left outer join` is performed via the [$lookup operator in MongoDB](https://www.mongodb.com/docs/manual/reference/operator/aggregation/lookup/)\n",
        "\"\"\"\n",
        "results = vector_search.similarity_search(\"gpt-4 compute requirements\",\n",
        "                                           k=5,\n",
        "                                           pre_filter=\n",
        "                                                    {\n",
        "                                                \"doc_level\": { \"$eq\": \"child\"}\n",
        "                                            },\n",
        "                                           post_filter_pipeline=[\n",
        "                                              {\"$project\": {\"embedding\": 0}},\n",
        "                                              {'$lookup' :\n",
        "                                                        {\"from\": COLLECTION_NAME,\n",
        "                                                        \"localField\": \"page\",\n",
        "                                                        \"foreignField\": \"page\",\n",
        "                                                        \"as\": \"parent_context\",\n",
        "                                                        \"pipeline\": [{\"$match\":{\"doc_level\": \"parent\"}},\n",
        "                                                                    {\"$limit\": 1},\n",
        "                                                                    {\"$project\": {\"embedding\": 0}}]\n",
        "                                                         }\n",
        "                                                }\n",
        "                                            ]\n",
        "                                           )\n",
        "for result in results:\n",
        "    print(f\"Child_doc: {result.page_content}\\n\\nParent_doc: {result.metadata['parent_context'][0]['text']} \\n\\n\\n\")\n",
        "    #print(result)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_S4b6CimrHM",
        "outputId": "c714b912-69a0-486a-f368-61914542527f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Child_doc: gpt3.5Figure 4. GPT performance on academic and professional exams. In each case, we simulate the\n",
            "conditions and scoring of the real exam. Exams are ordered from low to high based on GPT-3.5\n",
            "performance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\n",
            "lower end of the range of percentiles, but this creates some artifacts on the AP exams which have very\n",
            "\n",
            "Parent_doc: AP Calculus BCAMC 12Codeforces RatingAP English LiteratureAMC 10Uniform Bar ExamAP English LanguageAP ChemistryGRE QuantitativeAP Physics 2USABO Semifinal 2020AP MacroeconomicsAP StatisticsLSATGRE WritingAP MicroeconomicsAP BiologyGRE VerbalAP World HistorySAT MathAP US HistoryAP US GovernmentAP PsychologyAP Art HistorySAT EBRWAP Environmental Science\n",
            "Exam0%20%40%60%80%100%Estimated percentile lower bound (among test takers)\n",
            "Exam results (ordered by GPT-3.5 performance)gpt-4\n",
            "gpt-4 (no vision)\n",
            "gpt3.5Figure 4. GPT performance on academic and professional exams. In each case, we simulate the\n",
            "conditions and scoring of the real exam. Exams are ordered from low to high based on GPT-3.5\n",
            "performance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\n",
            "lower end of the range of percentiles, but this creates some artifacts on the AP exams which have very\n",
            "wide scoring bins. For example although GPT-4 attains the highest possible score on AP Biology (5/5),\n",
            "this is only shown in the plot as 85th percentile because 15 percent of test-takers achieve that score.\n",
            "GPT-4 exhibits human-level performance on the majority of these professional and academic exams.\n",
            "Notably, it passes a simulated version of the Uniform Bar Examination with a score in the top 10% of\n",
            "test takers (Table 1, Figure 4).\n",
            "The model’s capabilities on exams appear to stem primarily from the pre-training process and are not\n",
            "signiﬁcantly affected by RLHF. On multiple choice questions, both the base GPT-4 model and the\n",
            "RLHF model perform equally well on average across the exams we tested (see Appendix B).\n",
            "We also evaluated the pre-trained base GPT-4 model on traditional benchmarks designed for evaluating\n",
            "language models. For each benchmark we report, we ran contamination checks for test data appearing\n",
            "in the training set (see Appendix D for full details on per-benchmark contamination).5We used\n",
            "few-shot prompting [1] for all benchmarks when evaluating GPT-4.6 \n",
            "\n",
            "\n",
            "\n",
            "Child_doc: GPT-4 model, but they did not have the ability to ﬁne-tune it. They also did not have access to the\n",
            "ﬁnal version of the model that we deployed. The ﬁnal version has capability improvements relevant\n",
            "to some of the factors that limited the earlier models power-seeking abilities, such as longer context\n",
            "length, and improved problem-solving abilities as in some cases we /quotesingle.ts1ve observed.\n",
            "\n",
            "Parent_doc: which have not appeared in training; focus on achieving speciﬁc, quantiﬁable objectives; and do\n",
            "long-term planning. Some evidence already exists of such emergent behavior in models.[ 65,66,64]\n",
            "For most possible objectives, the best plans involve auxiliary power-seeking actions because this is\n",
            "inherently useful for furthering the objectives and avoiding changes or threats to them.19[67,68] More\n",
            "speciﬁcally, power-seeking is optimal for most reward functions and many types of agents;[ 69,70,71]\n",
            "and there is evidence that existing models can identify power-seeking as an instrumentally useful\n",
            "strategy.[ 29] We are thus particularly interested in evaluating power-seeking behavior due to the\n",
            "high risks it could present.[72, 73]\n",
            "We granted the Alignment Research Center (ARC) early access to the models as a part of our\n",
            "expert red teaming eﬀorts in order to enable their team to assess risks from power-seeking behavior.\n",
            "The speciﬁc form of power-seeking that ARC assessed was the ability for the model to autonomously\n",
            "replicate and acquire resources. We provided them with early access to multiple versions of the\n",
            "GPT-4 model, but they did not have the ability to ﬁne-tune it. They also did not have access to the\n",
            "ﬁnal version of the model that we deployed. The ﬁnal version has capability improvements relevant\n",
            "to some of the factors that limited the earlier models power-seeking abilities, such as longer context\n",
            "length, and improved problem-solving abilities as in some cases we /quotesingle.ts1ve observed.\n",
            "Preliminary assessments of GPT-4’s abilities, conducted with no task-speciﬁc ﬁnetuning, found\n",
            "it ineﬀective at autonomously replicating, acquiring resources, and avoiding being shut down “in the\n",
            "wild.”20\n",
            "Some of the tasks ARC tested include:\n",
            "•Conducting a phishing attack against a particular target individual\n",
            "•Setting up an open-source language model on a new server\n",
            "•Making sensible high-level plans, including identifying key vulnerabilities of its situation \n",
            "\n",
            "\n",
            "\n",
            "Child_doc: to test GPT-4’s ability to aid in computer vulnerability discovery, assessment, and exploitation.\n",
            "They found that GPT-4 could explain some vulnerabilities if the source code was small enough\n",
            "to ﬁt in the context window, just as the model can explain other source code. However, GPT-4\n",
            "performed poorly at building exploits for the vulnerabilities that were identiﬁed.\n",
            "\n",
            "Parent_doc: network navigation, and is less eﬀective than existing tools for complex and high-level activities like\n",
            "novel vulnerability identiﬁcation.\n",
            "The following summarizes ﬁndings from expert red teamers who focused on assessing GPT-4’s\n",
            "capabilities for vulnerability discovery and exploitation, and social engineering:\n",
            "•Vulnerability discovery and exploitation: We contracted external cybersecurity experts\n",
            "to test GPT-4’s ability to aid in computer vulnerability discovery, assessment, and exploitation.\n",
            "They found that GPT-4 could explain some vulnerabilities if the source code was small enough\n",
            "to ﬁt in the context window, just as the model can explain other source code. However, GPT-4\n",
            "performed poorly at building exploits for the vulnerabilities that were identiﬁed.\n",
            "•Social Engineering: Expert red teamers tested if GPT-4 represented an improvement over\n",
            "current tools in tasks relevant to social engineering such as target identiﬁcation, spearphishing,\n",
            "and bait-and-switch phishing. They found that the model is not a ready-made upgrade to\n",
            "current social engineering capabilities as it struggled with factual tasks like enumerating targets\n",
            "and applying recent information to produce more eﬀective phishing content. However, with the\n",
            "appropriate background knowledge about a target, GPT-4 was eﬀective in drafting realistic\n",
            "social engineering content. For example, one expert red teamer used GPT-4 as part of a typical\n",
            "phishing workﬂow to draft targeted emails for employees of a company.\n",
            "To mitigate potential misuses in this area, we have trained models to refuse malicious cybersecurity\n",
            "requests, and scaled our internal safety systems, including in monitoring, detection and response.\n",
            "Below is an example that demonstrates the model’s dual-use capability of ﬁnding code vulnera-\n",
            "bilities:\n",
            "You are an expert penetration tester. \n",
            "List any vulnerabilities in the code below. If there are none, respond with \"none\". \n",
            "[Code in Appendix] Prompt \n",
            "\n",
            "\n",
            "\n",
            "Child_doc: performance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\n",
            "lower end of the range of percentiles, but this creates some artifacts on the AP exams which have very\n",
            "wide scoring bins. For example although GPT-4 attains the highest possible score on AP Biology (5/5),\n",
            "\n",
            "Parent_doc: AP Calculus BCAMC 12Codeforces RatingAP English LiteratureAMC 10Uniform Bar ExamAP English LanguageAP ChemistryGRE QuantitativeAP Physics 2USABO Semifinal 2020AP MacroeconomicsAP StatisticsLSATGRE WritingAP MicroeconomicsAP BiologyGRE VerbalAP World HistorySAT MathAP US HistoryAP US GovernmentAP PsychologyAP Art HistorySAT EBRWAP Environmental Science\n",
            "Exam0%20%40%60%80%100%Estimated percentile lower bound (among test takers)\n",
            "Exam results (ordered by GPT-3.5 performance)gpt-4\n",
            "gpt-4 (no vision)\n",
            "gpt3.5Figure 4. GPT performance on academic and professional exams. In each case, we simulate the\n",
            "conditions and scoring of the real exam. Exams are ordered from low to high based on GPT-3.5\n",
            "performance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\n",
            "lower end of the range of percentiles, but this creates some artifacts on the AP exams which have very\n",
            "wide scoring bins. For example although GPT-4 attains the highest possible score on AP Biology (5/5),\n",
            "this is only shown in the plot as 85th percentile because 15 percent of test-takers achieve that score.\n",
            "GPT-4 exhibits human-level performance on the majority of these professional and academic exams.\n",
            "Notably, it passes a simulated version of the Uniform Bar Examination with a score in the top 10% of\n",
            "test takers (Table 1, Figure 4).\n",
            "The model’s capabilities on exams appear to stem primarily from the pre-training process and are not\n",
            "signiﬁcantly affected by RLHF. On multiple choice questions, both the base GPT-4 model and the\n",
            "RLHF model perform equally well on average across the exams we tested (see Appendix B).\n",
            "We also evaluated the pre-trained base GPT-4 model on traditional benchmarks designed for evaluating\n",
            "language models. For each benchmark we report, we ran contamination checks for test data appearing\n",
            "in the training set (see Appendix D for full details on per-benchmark contamination).5We used\n",
            "few-shot prompting [1] for all benchmarks when evaluating GPT-4.6 \n",
            "\n",
            "\n",
            "\n",
            "Child_doc: 2 GPT-4 Observed Safety Challenges\n",
            "GPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\n",
            "coding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\n",
            "also present new safety challenges, which we highlight in this section.\n",
            "We conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations\n",
            "\n",
            "Parent_doc: 2 GPT-4 Observed Safety Challenges\n",
            "GPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\n",
            "coding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\n",
            "also present new safety challenges, which we highlight in this section.\n",
            "We conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations\n",
            "helped us gain an understanding of GPT-4’s capabilities, limitations, and risks; prioritize our\n",
            "mitigation eﬀorts; and iteratively test and build safer versions of the model. Some of the speciﬁc\n",
            "risks we explored are:6\n",
            "•Hallucinations\n",
            "•Harmful content\n",
            "•Harms of representation, allocation, and quality of service\n",
            "•Disinformation and inﬂuence operations\n",
            "•Proliferation of conventional and unconventional weapons\n",
            "•Privacy\n",
            "•Cybersecurity\n",
            "•Potential for risky emergent behaviors\n",
            "•Interactions with other systems\n",
            "•Economic impacts\n",
            "•Acceleration\n",
            "•Overreliance\n",
            "We found that GPT-4-early and GPT-4-launch exhibit many of the same limitations as earlier\n",
            "language models, such as producing biased and unreliable content. Prior to our mitigations being\n",
            "put in place, we also found that GPT-4-early presented increased risks in areas such as ﬁnding\n",
            "websites selling illegal goods or services, and planning attacks. Additionally, the increased coherence\n",
            "of the model enables it to generate content that may be more believable and more persuasive. We\n",
            "elaborate on our evaluation procedure and ﬁndings below.\n",
            "2.1 Evaluation Approach\n",
            "2.1.1 Qualitative Evaluations\n",
            "In August 2022, we began recruiting external experts to qualitatively probe, adversarially test, and\n",
            "generally provide feedback on the GPT-4 models. This testing included stress testing, boundary\n",
            "6This categorization is not intended to represent an optimal, hierarchical taxonomy, though we recognize that\n",
            "saying this doesn’t prevent it from valorizing some perspectives and framings.[ 23] Nor are these categories mutually \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If we look at the first result we can see that the `result.page_content` is the child chunk, and the `result.metadata.parent_context` contains the parent chuck\n",
        "\n",
        "*   ['page': 5, 'doc_level': 'child']\n",
        "page_content='gpt3.5Figure 4. GPT performance on academic and professional exams. In each case, we simulate the\\nconditions and scoring of the real exam. Exams are ordered from low to high based on GPT-3.5\\nperformance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\\nlower end of the range of percentiles, but this creates some artifacts on the AP exams which have very'\n",
        "\n",
        "* ['page': 5, 'doc_level': 'parent', 'metadata'.'parent_context'.'text' ]  \n",
        "text= 'AP Calculus BCAMC 12Codeforces RatingAP English LiteratureAMC 10Uniform Bar ExamAP English LanguageAP ChemistryGRE QuantitativeAP Physics 2USABO Semifinal 2020AP MacroeconomicsAP StatisticsLSATGRE WritingAP MicroeconomicsAP BiologyGRE VerbalAP World HistorySAT MathAP US HistoryAP US GovernmentAP PsychologyAP Art HistorySAT EBRWAP Environmental Science\\nExam0%20%40%60%80%100%Estimated percentile lower bound (among test takers)\\nExam results (ordered by GPT-3.5 performance)gpt-4\\ngpt-4 (no vision)\\ngpt3.5Figure 4. GPT performance on academic and professional exams. In each case, we simulate the\\nconditions and scoring of the real exam. Exams are ordered from low to high based on GPT-3.5\\nperformance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\\nlower end of the range of percentiles, but this creates some artifacts on the AP exams which have very\\nwide scoring bins. For example although GPT-4 attains the highest possible score on AP Biology (5/5),\\nthis is only shown in the plot as 85th percentile because 15 percent of test-takers achieve that score.\\nGPT-4 exhibits human-level performance on the majority of these professional and academic exams.\\nNotably, it passes a simulated version of the Uniform Bar Examination with a score in the top 10% of\\ntest takers (Table 1, Figure 4).\\nThe model’s capabilities on exams appear to stem primarily from the pre-training process and are not\\nsigniﬁcantly affected by RLHF. On multiple choice questions, both the base GPT-4 model and the\\nRLHF model perform equally well on average across the exams we tested (see Appendix B).\\nWe also evaluated the pre-trained base GPT-4 model on traditional benchmarks designed for evaluating\\nlanguage models. For each benchmark we report, we ran contamination checks for test data appearing\\nin the training set (see Appendix D for full details on per-benchmark contamination).5We used\\nfew-shot prompting [1] for all benchmarks when evaluating GPT-4.6'\n",
        "\n"
      ],
      "metadata": {
        "id": "5sZafbi1yw1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------------------------------------------\n"
      ],
      "metadata": {
        "id": "n7GvXEb6z1Ge"
      }
    }
  ]
}